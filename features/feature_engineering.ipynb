{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hft4Q645OGUm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "import sklearn.processing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHeINj4IUM-5"
      },
      "source": [
        "## 1. Population Density Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yreqRmmfQ3R_"
      },
      "outputs": [],
      "source": [
        "df_population_2021 = pd.read_csv(\"data/processed/df_population_2021.csv\")\n",
        "df_population_2022 = pd.read_csv(\"data/processed/df_population_2022.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-F6DrG5ujJ4"
      },
      "outputs": [],
      "source": [
        "df_pop_unique = (\n",
        "    df_population_2021[['fips_code', 'population_density']]\n",
        "    .drop_duplicates(subset='fips_code')\n",
        ")\n",
        "\n",
        "# join by fips_code\n",
        "df_outages_2021 = pd.merge(\n",
        "    df_outages_2021,\n",
        "    df_pop_unique,\n",
        "    on='fips_code',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(df_outages_2021.head())\n",
        "print(df_outages_2021.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pviXeC3Muorj"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_pop_unique = (\n",
        "    df_population_2022[['fips_code', 'population_density']]\n",
        "    .drop_duplicates(subset='fips_code')\n",
        ")\n",
        "\n",
        "# join by fips_code\n",
        "df_outages_2022 = pd.merge(\n",
        "    df_outages_2022,\n",
        "    df_pop_unique,\n",
        "    on='fips_code',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(df_outages_2022.head())\n",
        "print(df_outages_2022.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Outage Proxy Features (Rolling Stats)"
      ],
      "metadata": {
        "id": "jl6-LGLYyYmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rolling mean 12 h\n",
        "df_outages_2021['rolling_mean_12h'] = (\n",
        "    df_outages_2021\n",
        "    .groupby('fips_code')['sum']\n",
        "    .transform(lambda x: x.rolling(window=12, min_periods=1).mean())\n",
        ")\n",
        "\n",
        "# rolling max 12 h\n",
        "df_outages_2021['rolling_max_12h'] = (\n",
        "    df_outages_2021\n",
        "    .groupby('fips_code')['sum']\n",
        "    .transform(lambda x: x.rolling(window=12, min_periods=1).max())\n",
        ")"
      ],
      "metadata": {
        "id": "pShWGOvsyUg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rolling mean 12 h\n",
        "df_outages_2022['rolling_mean_12h'] = (\n",
        "    df_outages_2022\n",
        "    .groupby('fips_code')['sum']\n",
        "    .transform(lambda x: x.rolling(window=12, min_periods=1).mean())\n",
        ")\n",
        "\n",
        "# rolling max 12 h\n",
        "df_outages_2022['rolling_max_12h'] = (\n",
        "    df_outages_2022\n",
        "    .groupby('fips_code')['sum']\n",
        "    .transform(lambda x: x.rolling(window=12, min_periods=1).max())\n",
        ")"
      ],
      "metadata": {
        "id": "oHm-jW_QzHIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Day of Week"
      ],
      "metadata": {
        "id": "OvHsJ0Jjy6GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_outages_2021['day_of_week_num'] = (\n",
        "    df_outages_2021['run_start_time'].dt.dayofweek\n",
        ")\n",
        "df_outages_2022['day_of_week_num'] = (\n",
        "    df_outages_2022['run_start_time'].dt.dayofweek\n",
        ")"
      ],
      "metadata": {
        "id": "EOdL0n11y-SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5BTi9iyQIcD"
      },
      "source": [
        "## 4. Weather Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAN9NXAYPjSi"
      },
      "outputs": [],
      "source": [
        "df_weather_data_2021 = pd.read_csv(\"data/processed/merged_weather_data_2021.csv\")\n",
        "df_weather_data_2022 = pd.read_csv(\"data/processed/merged_weather_data_2022.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOw0SCIWCU-R"
      },
      "outputs": [],
      "source": [
        "df_weather_data_2021['valid'] = pd.to_datetime(df_weather_data_2021['valid'], errors='coerce')\n",
        "df_weather_data_2021.sort_values(['Name', 'valid'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5jcqybzDsLz"
      },
      "outputs": [],
      "source": [
        "df_weather_data_2022['valid'] = pd.to_datetime(df_weather_data_2022['valid'], errors='coerce')\n",
        "df_weather_data_2022.sort_values(['Name', 'valid'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDLlajyHRslt"
      },
      "outputs": [],
      "source": [
        "# remane columns\n",
        "df_weather_data_2021.rename(\n",
        "    columns={\n",
        "        'pred_dwpf': 'dwpf',\n",
        "        'pred_tmpf': 'tmpf',\n",
        "        'pred_relh': 'relh',\n",
        "        'gradient_value': 'relh_grad',\n",
        "        'pred_alti': 'alti',\n",
        "        'max_gust': 'gust',\n",
        "        'pred_sknt': 'sknt',\n",
        "        'pred_u': 'drct_u',\n",
        "        'pred_v': 'drct_v',\n",
        "        'pred_mslp': 'mslp'\n",
        "    },\n",
        "    inplace=True\n",
        ")\n",
        "df_weather_data_2021.drop(columns=['x_c', 'y_c'], inplace=True)\n",
        "\n",
        "print(df_weather_data_2021.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48UICzr9augB"
      },
      "outputs": [],
      "source": [
        "# remane columns\n",
        "df_weather_data_2022.rename(\n",
        "    columns={\n",
        "        'pred_dwpf': 'dwpf',\n",
        "        'pred_tmpf': 'tmpf',\n",
        "        'pred_relh': 'relh',\n",
        "        'gradient_value': 'relh_grad',\n",
        "        'pred_alti': 'alti',\n",
        "        'max_gust': 'gust',\n",
        "        'pred_sknt': 'sknt',\n",
        "        'pred_u': 'drct_u',\n",
        "        'pred_v': 'drct_v',\n",
        "        'pred_mslp': 'mslp'\n",
        "    },\n",
        "    inplace=True\n",
        ")\n",
        "df_weather_data_2022.drop(columns=['x_c', 'y_c'], inplace=True)\n",
        "\n",
        "print(df_weather_data_2022.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leliQ4EPYH8f"
      },
      "outputs": [],
      "source": [
        "# convert to datetime format\n",
        "df_outages_2021['run_start_time'] = pd.to_datetime(df_outages_2021['run_start_time'])\n",
        "df_weather_data_2021['valid'] = pd.to_datetime(df_weather_data_2021['valid'])\n",
        "\n",
        "# merge, keep all raws present in df_outages_2021\n",
        "df_final_2021 = pd.merge(\n",
        "    df_outages_2021,\n",
        "    df_weather_data_2021,\n",
        "    left_on=['run_start_time', 'county'],\n",
        "    right_on=['valid', 'Name'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "df_final_2021.drop(columns=['valid', 'county'], inplace=True)\n",
        "\n",
        "print(df_final_2021.info())\n",
        "print(df_final_2021.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUESLM_Fa1a-"
      },
      "outputs": [],
      "source": [
        "# convert to datetime format\n",
        "df_outages_2022['run_start_time'] = pd.to_datetime(df_outages_2022['run_start_time'])\n",
        "df_weather_data_2022['valid'] = pd.to_datetime(df_weather_data_2022['valid'])\n",
        "\n",
        "# merge, keep all raws present in df_outages_2022\n",
        "df_final_2022 = pd.merge(\n",
        "    df_outages_2022,\n",
        "    df_weather_data_2022,\n",
        "    left_on=['run_start_time', 'county'],\n",
        "    right_on=['valid', 'Name'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "df_final_2022.drop(columns=['valid', 'county'], inplace=True)\n",
        "\n",
        "print(df_final_2022.info())\n",
        "print(df_final_2022.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA0yyF3tnSXy"
      },
      "source": [
        "## 5. Weather Temporal Aggregates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysPAstRjiDNH"
      },
      "outputs": [],
      "source": [
        "# sort each county (fips_code) by chronological order\n",
        "df_final_2021.sort_values(by=['fips_code', 'run_start_time'], inplace=True)\n",
        "\n",
        "\n",
        "# lags for parameters where sudden changes matter\n",
        "weather_cols_lag = ['dwpf', 'tmpf', 'drct_u', 'drct_v']\n",
        "lag_hours = [6, 12, 24, 48]\n",
        "\n",
        "for col in weather_cols_lag:\n",
        "    for lag in lag_hours:\n",
        "        df_final_2021[f'{col}_lag_{lag}h'] = df_final_2021.groupby(['fips_code'])[col].shift(lag)\n",
        "\n",
        "\n",
        "# rolling sum for accumulated metrics\n",
        "weather_cols_rolling_sum = ['p0li']\n",
        "rolling_windows_sum = [6, 12, 24, 48]\n",
        "\n",
        "for col in weather_cols_rolling_sum:\n",
        "    for window in rolling_windows_sum:\n",
        "        df_final_2021[f'{col}_rolling_sum_{window}h'] = (\n",
        "            df_final_2021.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .sum()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "\n",
        "# rolling mean to capture trends\n",
        "weather_cols_rolling_mean = ['alti', 'mslp', 'relh']\n",
        "rolling_windows_mean = [6, 12, 24, 48]\n",
        "\n",
        "for col in weather_cols_rolling_mean:\n",
        "    for window in rolling_windows_mean:\n",
        "        df_final_2021[f'{col}_rolling_mean_{window}h'] = (\n",
        "            df_final_2021.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .mean()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "\n",
        "# rolling max to capture peak values\n",
        "weather_cols_rolling_max = ['gust', 'sknt', 'relh_grad']\n",
        "rolling_windows_max = [6, 12, 24, 48]\n",
        "\n",
        "for col in weather_cols_rolling_max:\n",
        "    for window in rolling_windows_max:\n",
        "        df_final_2021[f'{col}_rolling_max_{window}h'] = (\n",
        "            df_final_2021.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .max()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "# lags and rolling sum for binary flags\n",
        "binary_flags = ['ts_flag', 'hr_flag', 'sq_flag']\n",
        "rolling_windows_flag = [6, 12, 24, 48]\n",
        "\n",
        "for col in binary_flags:\n",
        "    for window in rolling_windows_flag:\n",
        "        df_final_2021[f'{col}_rolling_sum_{window}h'] = (\n",
        "            df_final_2021.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .sum()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "df_final_2021.reset_index(drop=True, inplace=True)\n",
        "print(df_final_2021.info())\n",
        "print(df_final_2021.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1EfhjdZnJRm"
      },
      "outputs": [],
      "source": [
        "# sort each county (fips_code) by chronological order\n",
        "df_final_2022.sort_values(by=['fips_code', 'run_start_time'], inplace=True)\n",
        "\n",
        "# lags for parameters where sudden changes matter\n",
        "weather_cols_lag = ['dwpf', 'tmpf', 'drct_u', 'drct_v']\n",
        "lag_hours = [6, 12, 24, 48]\n",
        "\n",
        "for col in weather_cols_lag:\n",
        "    for lag in lag_hours:\n",
        "        df_final_2022[f'{col}_lag_{lag}h'] = df_final_2022.groupby(['fips_code'])[col].shift(lag)\n",
        "\n",
        "# rolling sum for accumulated metrics\n",
        "weather_cols_rolling_sum = ['p0li']\n",
        "rolling_windows_sum = [6, 12, 24, 48]\n",
        "\n",
        "for col in weather_cols_rolling_sum:\n",
        "    for window in rolling_windows_sum:\n",
        "        df_final_2022[f'{col}_rolling_sum_{window}h'] = (\n",
        "            df_final_2022.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .sum()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "# rolling mean to capture trends\n",
        "weather_cols_rolling_mean = ['alti', 'mslp', 'relh']\n",
        "rolling_windows_mean = [6, 12, 24, 48]  #12, 24. 6, 12\n",
        "\n",
        "for col in weather_cols_rolling_mean:\n",
        "    for window in rolling_windows_mean:\n",
        "        df_final_2022[f'{col}_rolling_mean_{window}h'] = (\n",
        "            df_final_2022.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .mean()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "# rolling max to capture peak values\n",
        "weather_cols_rolling_max = ['gust', 'sknt', 'relh_grad']\n",
        "rolling_windows_max = [6, 12, 24, 48]  #12, 24. 6, 12\n",
        "\n",
        "for col in weather_cols_rolling_max:\n",
        "    for window in rolling_windows_max:\n",
        "        df_final_2022[f'{col}_rolling_max_{window}h'] = (\n",
        "            df_final_2022.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .max()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "# lags and rolling sum for binary flags\n",
        "binary_flags = ['ts_flag', 'hr_flag', 'sq_flag']\n",
        "rolling_windows_flag = [6, 12, 24, 48]  #12, 24. 6, 12\n",
        "\n",
        "for col in binary_flags:\n",
        "    for window in rolling_windows_flag:\n",
        "        df_final_2022[f'{col}_rolling_sum_{window}h'] = (\n",
        "            df_final_2022.groupby(['fips_code'])[col]\n",
        "              .rolling(window=window, min_periods=1)\n",
        "              .sum()\n",
        "              .reset_index(level=['fips_code'], drop=True)\n",
        "        )\n",
        "\n",
        "df_final_2022.reset_index(drop=True, inplace=True)\n",
        "print(df_final_2022.info())\n",
        "print(df_final_2022.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHifneDzWjih"
      },
      "source": [
        "## 6. Target Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZynP9MaH1tNj"
      },
      "outputs": [],
      "source": [
        "# Target difinition:\n",
        "# 1. Define anomaly based on 90Q with MinMax normalization\n",
        "# 2. 48H shift for anomaly_flag\n",
        "# 3. 48H shift for sum\n",
        "\n",
        "df_2021 = df_final_2021.copy()\n",
        "\n",
        "# 1) sort\n",
        "df_2021.sort_values(['fips_code','run_start_time'], inplace=True)\n",
        "\n",
        "# 2) min and max of 'sum' for each fips_code\n",
        "df_2021['sum_min'] = df_2021.groupby('fips_code')['sum'].transform('min')\n",
        "df_2021['sum_max'] = df_2021.groupby('fips_code')['sum'].transform('max')\n",
        "\n",
        "# 3) MinMax scaler\n",
        "df_2021['sum_norm'] = np.where(\n",
        "    df_2021['sum_min'] != df_2021['sum_max'],\n",
        "    (df_2021['sum'] - df_2021['sum_min']) / (df_2021['sum_max'] - df_2021['sum_min']),\n",
        "    0.0\n",
        ")\n",
        "\n",
        "# 4) 0.99 Q of sum_norm for each fips_code\n",
        "q = 0.90\n",
        "quantile_stats_2021 = (\n",
        "    df_2021.groupby('fips_code')['sum_norm']\n",
        "           .quantile(q)\n",
        "           .reset_index(name=f'quantile_{int(q*100)}')\n",
        ")\n",
        "\n",
        "# 5) merge Quantile stats back\n",
        "df_2021 = pd.merge(df_2021, quantile_stats_2021, on='fips_code', how='left')\n",
        "\n",
        "# 6) anomaly_flag: 1 if sum_norm >= quantile\n",
        "df_2021['anomaly_flag'] = (\n",
        "    df_2021['sum_norm'] >= df_2021[f'quantile_{int(q*100)}']\n",
        ").astype(int)\n",
        "\n",
        "# 7) drop columns\n",
        "cols_drop = [\n",
        "    'sum_min','sum_max','sum_norm',\n",
        "    f'quantile_{int(q*100)}'\n",
        "]\n",
        "df_2021.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# 8) second stage\n",
        "df_2021_binary = df_2021.copy()\n",
        "\n",
        "# 9) time_48h = +48 часов\n",
        "df_2021_binary['time_48h'] = df_2021_binary['run_start_time'] + pd.Timedelta(hours=48)\n",
        "\n",
        "# 10) prepare df_shifted_2021\n",
        "df_shifted_2021 = df_2021_binary[['Name','run_start_time','sum','anomaly_flag']].copy()\n",
        "df_shifted_2021.rename(columns={\n",
        "    'run_start_time': 'time_48h_ahead',\n",
        "    'sum': 'sum_48',\n",
        "    'anomaly_flag': 'target_anomaly_48h'\n",
        "}, inplace=True)\n",
        "\n",
        "# 11) merge\n",
        "df_2021_binary = pd.merge(\n",
        "    df_2021_binary,\n",
        "    df_shifted_2021,\n",
        "    left_on=['Name','time_48h'],\n",
        "    right_on=['Name','time_48h_ahead'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 12) fillna для sum_48 / target_anomaly_48h\n",
        "df_2021_binary['sum_48'] = df_2021_binary['sum_48'].fillna(0)\n",
        "df_2021_binary['target_anomaly_48h'] = df_2021_binary['target_anomaly_48h'].fillna(0).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "df_2021_binary.drop(columns=['time_48h','time_48h_ahead'], inplace=True, errors='ignore')\n",
        "\n",
        "print(\"=== 2021 final shape:\", df_2021_binary.shape)\n",
        "print(df_2021_binary[['Name','run_start_time','anomaly_flag','target_anomaly_48h','sum_48']].head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w94Rr6RNWWB0"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_2022 = df_final_2022.copy()\n",
        "\n",
        "# 1) sort\n",
        "df_2022.sort_values(['fips_code','run_start_time'], inplace=True)\n",
        "\n",
        "# 2) min and max of 'sum' for each fips_code\n",
        "df_2022['sum_min'] = df_2022.groupby('fips_code')['sum'].transform('min')\n",
        "df_2022['sum_max'] = df_2022.groupby('fips_code')['sum'].transform('max')\n",
        "\n",
        "# 3) MinMax scaler\n",
        "df_2022['sum_norm'] = np.where(\n",
        "    df_2022['sum_min'] != df_2022['sum_max'],\n",
        "    (df_2022['sum'] - df_2022['sum_min']) / (df_2022['sum_max'] - df_2022['sum_min']),\n",
        "    0.0\n",
        ")\n",
        "\n",
        "# 4) 0.99 Q of sum_norm for each fips_code\n",
        "q = 0.90\n",
        "quantile_stats_2022 = (\n",
        "    df_2022.groupby('fips_code')['sum_norm']\n",
        "           .quantile(q)\n",
        "           .reset_index(name=f'quantile_{int(q*100)}')\n",
        ")\n",
        "\n",
        "# 5) merge Quantile stats back\n",
        "df_2022 = pd.merge(df_2022, quantile_stats_2022, on='fips_code', how='left')\n",
        "\n",
        "# 6) anomaly_flag: 1 if sum_norm >= quantile\n",
        "df_2022['anomaly_flag'] = (\n",
        "    df_2022['sum_norm'] >= df_2022[f'quantile_{int(q*100)}']\n",
        ").astype(int)\n",
        "\n",
        "# 7) drop columns\n",
        "cols_drop = [\n",
        "    'sum_min','sum_max','sum_norm',\n",
        "    f'quantile_{int(q*100)}'\n",
        "]\n",
        "df_2022.drop(columns=cols_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# 8) second stage\n",
        "df_2022_binary = df_2022.copy()\n",
        "\n",
        "# 9) time_48h = +48 часов\n",
        "df_2022_binary['time_48h'] = df_2022_binary['run_start_time'] + pd.Timedelta(hours=48)\n",
        "\n",
        "# 10) prepare df_shifted_2021\n",
        "df_shifted_2022 = df_2022_binary[['Name','run_start_time','sum','anomaly_flag']].copy()\n",
        "df_shifted_2022.rename(columns={\n",
        "    'run_start_time': 'time_48h_ahead',\n",
        "    'sum': 'sum_48',\n",
        "    'anomaly_flag': 'target_anomaly_48h'\n",
        "}, inplace=True)\n",
        "\n",
        "# 11) merge\n",
        "df_2022_binary = pd.merge(\n",
        "    df_2022_binary,\n",
        "    df_shifted_2022,\n",
        "    left_on=['Name','time_48h'],\n",
        "    right_on=['Name','time_48h_ahead'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 12) fillna для sum_48 / target_anomaly_48h\n",
        "df_2022_binary['sum_48'] = df_2022_binary['sum_48'].fillna(0)\n",
        "df_2022_binary['target_anomaly_48h'] = df_2022_binary['target_anomaly_48h'].fillna(0).astype(int)\n",
        "\n",
        "\n",
        "df_2022_binary.drop(columns=['time_48h','time_48h_ahead'], inplace=True, errors='ignore')\n",
        "\n",
        "print(\"=== 2022 final shape:\", df_2022_binary.shape)\n",
        "print(df_2022_binary[['Name','run_start_time','anomaly_flag','target_anomaly_48h','sum_48']].head(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud-PZTBjC6Qy"
      },
      "source": [
        "## 6. Vertical Concatenate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KepTWxBJC295"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_2021_binary.sort_values(by='run_start_time', inplace=True)\n",
        "df_2022_binary.sort_values(by='run_start_time', inplace=True)\n",
        "\n",
        "\n",
        "df = pd.concat([df_2021_binary, df_2022_binary], ignore_index=True)\n",
        "df.sort_values(by='run_start_time', inplace=True)\n",
        "df.info()\n",
        "print(df.head())\n",
        "print(df.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ROoIaoFm4A"
      },
      "source": [
        "## 7. IDW Aggregates"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_distance_matrix(df):\n",
        "    \"\"\"\n",
        "      dist_matrix[fips_i][fips_j] = Euclidean distance between counties (in meters)\n",
        "    \"\"\"\n",
        "    # extract counties based on 'fips_code'\n",
        "    unique_counties = df[['fips_code', 'x', 'y']].drop_duplicates().dropna(subset=['fips_code', 'x', 'y'])\n",
        "    codes = unique_counties['fips_code'].tolist()\n",
        "\n",
        "\n",
        "    # creates a dict for county's fips_code to its x, y\n",
        "    coords_dict = {row['fips_code']: (row['x'], row['y']) for idx, row in unique_counties.iterrows()}\n",
        "\n",
        "    # building the distance matrix for each county, computing the Euclidean distance to every other county\n",
        "    dist_matrix = {}\n",
        "    for code_i in codes:\n",
        "        dist_matrix[code_i] = {}\n",
        "        x_i, y_i = coords_dict[code_i]\n",
        "        for code_j in codes:\n",
        "            if code_i == code_j:\n",
        "                dist_matrix[code_i][code_j] = 0.0\n",
        "            else:\n",
        "                x_j, y_j = coords_dict[code_j]\n",
        "                dist_matrix[code_i][code_j] = math.hypot(x_i - x_j, y_i - y_j)\n",
        "    return dist_matrix"
      ],
      "metadata": {
        "id": "FUUd3ptN9tV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idw_feature(df, dist_matrix, feature, p=2):\n",
        "    \"\"\"\n",
        "    This function computes IDW weighted averaged values of neighboring counties.\n",
        "    The result is stored in a new column named \"IDW_{feature}\".\n",
        "\n",
        "    Parameters:\n",
        "      - df (Dataframe)\n",
        "      - dist_matrix (a distance dictionary obtained from (build_distance_matrix)\n",
        "      - feature (specified features)\n",
        "      - p (exponents for IDW weights weight =1/d^2 on inverse square law)\n",
        "    \"\"\"\n",
        "    # Sort the DataFrame by timestamp and county identifier\n",
        "    # Sort by timestamp and fips_code\n",
        "    df = df.sort_values(['run_start_time', 'fips_code']).copy()\n",
        "    results = []\n",
        "\n",
        "    # group by timestamp\n",
        "    for t, group in df.groupby('run_start_time'):\n",
        "        # create a dict for the current time slice\n",
        "        feature_dict = dict(zip(group['fips_code'], group[feature]))\n",
        "\n",
        "        # Process each row in the current time slice\n",
        "        for idx, row in group.iterrows():\n",
        "            current_code = row['fips_code']\n",
        "            sum_w = 0.0         # sum of weights\n",
        "            sum_weighted = 0.0  # sum of weights feature values\n",
        "\n",
        "            # iteration over all counties in the same time slice\n",
        "            for other_code, other_val in feature_dict.items():\n",
        "                if other_code == current_code:\n",
        "                    continue  # skip the current county itself\n",
        "\n",
        "                # get the distance from the current county to the other county\n",
        "                d = dist_matrix[current_code].get(other_code, None)\n",
        "                if d is not None and d > 0:\n",
        "                    w = 1.0 / (d ** p) # Compute the weight as 1/d^p\n",
        "                    sum_w += w\n",
        "                    sum_weighted += w * other_val\n",
        "\n",
        "            # Compute IDW if neighbours are found, otherwise assign NaN\n",
        "            idw_val = sum_weighted / sum_w if sum_w > 0 else np.nan\n",
        "            row[f\"IDW_{feature}\"] = idw_val\n",
        "            results.append(row)\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "A6kf8imL-FQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26IL3qkl0UeT"
      },
      "source": [
        "## 8. County Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j88AZRxdnDpv"
      },
      "outputs": [],
      "source": [
        "# Encoding County Name\n",
        "le = LabelEncoder()\n",
        "df_result['county_encoded'] = le.fit_transform(df_result['Name'])\n",
        "\n",
        "# save Labelencoded classes\n",
        "county_mapping = {county: int(idx) for idx, county in enumerate(le.classes_)}\n",
        "\n",
        "# Delete column 'county'\n",
        "df_result.drop(columns=['Name'], inplace=True)\n",
        "\n",
        "# Save JSON for future reconciliation\n",
        "with open('county_mapping.json', 'w') as f:\n",
        "    json.dump(county_mapping, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHqnK4Kh2PZ"
      },
      "source": [
        "## 9. NaN Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IFKJHRxp1xP"
      },
      "outputs": [],
      "source": [
        "df_result.dropna(inplace=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4JesAkmtOMKC",
        "rsotk1R3Oi4I",
        "R-ba1Gj5K06Z",
        "K-OwXD7iK6rK",
        "FJKl9TDwqXvR",
        "gnoNI54q-U3e",
        "ZHeINj4IUM-5",
        "HXj2UtBQ2xiv",
        "r0YJ9EMtflgB",
        "y5BTi9iyQIcD",
        "XHifneDzWjih",
        "b1ROoIaoFm4A",
        "e6-hdHJVPzlp",
        "cHMzyhlbzEzY",
        "26IL3qkl0UeT",
        "tpHqnK4Kh2PZ",
        "Vi8_kTucKkpH",
        "QUe2mQppiPqC",
        "2XCqVhYOx7V9",
        "t4fP9d6Q3tGO"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
